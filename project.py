# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qRy6bRgBBQAVUtzS3HjPiH9TRrJH0rwG

## **Computing Risk Parity Portfolio Weights**
*Authours: Madina Kudaibergenova and Madina Saparbayeva*

**Roadmap:** 
- Apply hierarchical clustering first
- Build a portfolio by picking one asset from each cluster (with equal weight),
- Compare this portfolio's risk with traditional methodologies for optimized portfolio.

<br>

**Data:** 10 assets   
["FB", "AMZN", "AAPL", "GOOG", "NFLX", "TSLA", "MSFT", "IBM", "ORCL", "ADBE"]

<br>

**Date:** almost 3 years   
[2019-01-04] to [2021-04-23]

<br>

**Results:**  
**Maximum Optimized Sharpe Ratio**  
Expected annual return : 98.0%  
Annual volatility/standard deviation/risk : 48.0%  
Annual variance : 23.0%  
<br>
**Sharpe Ratio Second Method**  
Expected annual return : 68.0%  
Annual volatility/standard deviation/risk : 35.0%  
Annual variance : 12.0%  
<br>
**HRP results**  
Expected annual return : 37.0%  
Annual volatility/standard deviation/risk : 31.0%  
Annual variance : 10.0%  

<br>

##Data Storage and Representation
"""

# Import the python libraries
from pandas_datareader import data as web
import pandas as pd
import numpy as np
from datetime import datetime
from tqdm import tnrange, tqdm_notebook, tqdm
from scipy.optimize import minimize
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
from sklearn.covariance import shrunk_covariance, ledoit_wolf, OAS, MinCovDet

#change it to need one 
assets =  ["FB", "AMZN", "AAPL", "GOOG", "NFLX", "TSLA", "MSFT", "IBM", "ORCL", "ADBE"]

# Assign weights to the stocks. Weights must = 1 so 0.1 for each
weights = np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])
weights

#Get the stock starting date and ending date in cvs files.
stockStartDate = '2019-01-04'
stockEndDate = '2021-04-23'

#Create a dataframe to store the close price of the stocks
close_prices = pd.DataFrame()

#Store the close price of stock into the data frame
for stock in assets:
   close_prices[stock] = web.DataReader(stock,data_source='yahoo',start=stockStartDate, end = stockEndDate)['Close']

returns = close_prices.pct_change()
close_prices

# Create the title 'Portfolio Close Price History
title = 'Portfolio Close Price History    '
#Get the stocks
my_stocks = close_prices
#Create and plot the graph
plt.figure(figsize=(12.2,4.5)) #width = 12.2in, height = 4.5
# Loop through each stock and plot the Close for each day
for c in my_stocks.columns.values:
  plt.plot( my_stocks[c],  label=c)#plt.plot( X-Axis , Y-Axis, line_width, alpha_for_blending,  label)
plt.title(title)
plt.xlabel('Date',fontsize=18)
plt.ylabel('Adj. Price USD ($)',fontsize=18)
plt.legend(my_stocks.columns.values, loc='upper left')
plt.show()# Create the title 'Portfolio Close Price History
title = 'Portfolio Close Price History    '

#Create normalized plot too see the variance of prices 
normalized_prices = close_prices.divide(close_prices.iloc[0] / 100)

plt.figure(figsize=(15, 6))
for i in range(normalized_prices.shape[1]):
    plt.plot(normalized_prices.iloc[:,i], label=normalized_prices.columns.values[i])
plt.legend(loc='upper left', fontsize=12)
plt.ylabel('Normalized prices')
plt.show()

"""##Financial Calculation"""

returns

#co-variance matrix to see the how much two variables vary or move together
#diagonal values are variance and others are co-variance
cov_matrix_annual = returns.cov() * 252 # 252 - the working day in year
cov_matrix_annual

def get_calc(given_weight):
  #Expected portfolio variance= WT * (Covariance Matrix) * W
  port_variance = np.dot(given_weight.T, np.dot(cov_matrix_annual, given_weight))

  #Expected portfolio volatility= SQRT (WT * (Covariance Matrix) * W)
  port_volatility = np.sqrt(port_variance)
  portfolioSimpleAnnualReturn = np.sum(returns.mean()*given_weight) * 252
  percent_var = str(round(port_variance, 2) * 100) + '%'
  percent_vols = str(round(port_volatility, 2) * 100) + '%'
  percent_ret = str(round(portfolioSimpleAnnualReturn, 2)*100)+'%'
  print("Expected annual return : "+ percent_ret)
  print('Annual volatility/standard deviation/risk : '+percent_vols)
  print('Annual variance : '+percent_var)

print("Equal Weigth: ") 
get_calc(weights)

"""## Marcos Lopez de Prado's hierarchical risk parity"""

import scipy.cluster.hierarchy as sch
import numpy as np
import pandas as pd
from datetime import date
from matplotlib import pyplot as plt
import cvxopt as opt
from cvxopt import blas, solvers
#https://quantdare.com/hierarchical-risk-parity/
#https://github.com/lcamposgarrido/data-science_projects/blob/master/others/hierarchical_risk_parity/HRP.ipynb

# On 20151227 by MLdP <lopezdeprado@lbl.gov>
# Hierarchical Risk Parity


def getIVP(cov, **kargs):
    # Compute the inverse-variance portfolio
    ivp = 1. / np.diag(cov)
    ivp /= ivp.sum()
    return ivp


def getClusterVar(cov,cItems):
    # Compute variance per cluster
    cov_=cov.loc[cItems,cItems] # matrix slice
    w_=getIVP(cov_).reshape(-1,1)
    cVar=np.dot(np.dot(w_.T,cov_),w_)[0,0]
    return cVar


def getQuasiDiag(link):
    # Sort clustered items by distance
    link = link.astype(int)
    sortIx = pd.Series([link[-1, 0], link[-1, 1]])
    numItems = link[-1, 3]  # number of original items
    while sortIx.max() >= numItems:
        sortIx.index = range(0, sortIx.shape[0] * 2, 2)  # make space
        df0 = sortIx[sortIx >= numItems]  # find clusters
        i = df0.index
        j = df0.values - numItems
        sortIx[i] = link[j, 0]  # item 1
        df0 = pd.Series(link[j, 1], index=i + 1)
        sortIx = sortIx.append(df0)  # item 2
        sortIx = sortIx.sort_index()  # re-sort
        sortIx.index = range(sortIx.shape[0])  # re-index
    return sortIx.tolist()


def getRecBipart(cov, sortIx):
    # Compute HRP alloc
    w = pd.Series(1, index=sortIx)
    cItems = [sortIx]  # initialize all items in one cluster
    while len(cItems) > 0:
        cItems = [i[j:k] for i in cItems for j, k in ((0, len(i) // 2), (len(i) // 2, len(i))) if len(i) > 1]  # bi-section
        for i in range(0, len(cItems), 2):  # parse in pairs
            cItems0 = cItems[i]  # cluster 1
            cItems1 = cItems[i + 1]  # cluster 2
            cVar0 = getClusterVar(cov, cItems0)
            cVar1 = getClusterVar(cov, cItems1)
            alpha = 1 - cVar0 / (cVar0 + cVar1)
            w[cItems0] *= alpha  # weight 1
            w[cItems1] *= 1 - alpha  # weight 2
    return w


def correlDist(corr):
    # A distance matrix based on correlation, where 0<=d[i,j]<=1
    # This is a proper distance metric
    dist = ((1 - corr) / 2.)**.5  # distance matrix
    return dist


def getHRP(cov, corr):
    # Construct a hierarchical portfolio
    dist = correlDist(corr)
    link = sch.linkage(dist, 'single')
    dn = sch.dendrogram(link, labels=cov.index.values)
    plt.show()
    sortIx = getQuasiDiag(link)
    sortIx = corr.index[sortIx].tolist()
    hrp = getRecBipart(cov, sortIx)
    return hrp.sort_index()

def getMVP(cov):

    cov = cov.T.values
    n = len(cov)
    N = 100
    mus = [10 ** (5.0 * t / N - 1.0) for t in range(N)]

    # Convert to cvxopt matrices
    S = opt.matrix(cov)
    #pbar = opt.matrix(np.mean(returns, axis=1))
    pbar = opt.matrix(np.ones(cov.shape[0]))

    # Create constraint matrices
    G = -opt.matrix(np.eye(n))  # negative n x n identity matrix
    h = opt.matrix(0.0, (n, 1))
    A = opt.matrix(1.0, (1, n))
    b = opt.matrix(1.0)

    # Calculate efficient frontier weights using quadratic programming
    portfolios = [solvers.qp(mu * S, -pbar, G, h, A, b)['x']
                  for mu in mus]
    ## CALCULATE RISKS AND RETURNS FOR FRONTIER
    returns = [blas.dot(pbar, x) for x in portfolios]
    risks = [np.sqrt(blas.dot(x, S * x)) for x in portfolios]
    ## CALCULATE THE 2ND DEGREE POLYNOMIAL OF THE FRONTIER CURVE
    m1 = np.polyfit(returns, risks, 2)
    x1 = np.sqrt(m1[2] / m1[0])
    # CALCULATE THE OPTIMAL PORTFOLIO
    wt = solvers.qp(opt.matrix(x1 * S), -pbar, G, h, A, b)['x']

    return list(wt)

def get_all_portfolios(returns):
    
    cov, corr = returns.cov(), returns.corr()
    hrp = getHRP(cov, corr)
    ivp = getIVP(cov)
    ivp = pd.Series(ivp, index=cov.index)
    mvp = getMVP(cov)
    mvp = pd.Series(mvp, index=cov.index)
    
    portfolios = pd.DataFrame([mvp, ivp, hrp], index=['MVP', 'IVP', 'HRP']).T
    
    return portfolios

portfolios = get_all_portfolios(returns)

portfolios

cov, corr = returns.cov(), returns.corr()
hrp = getHRP(cov, corr)
#link = sch.linkage(correlDist(hrp), 'single')
#sch.dendrogram(link, labels=df_returns.columns, distance_sort='descending', orientation='top')

"""##Regular Sharpe Ratio Optimized Portfolio"""

pip install PyPortfolioOpt

from pypfopt.efficient_frontier import EfficientFrontier
from pypfopt import risk_models
from pypfopt import expected_returns

df = close_prices
mu = expected_returns.mean_historical_return(df)#returns.mean() * 252
S = risk_models.sample_cov(df) #Get the sample covariance matrix

ef = EfficientFrontier(mu, S)
weights = ef.max_sharpe() #Maximize the Sharpe ratio, and get the raw weights
cleaned_weights = ef.clean_weights() 
print(cleaned_weights) #Note the weights may have some rounding error, meaning they may not add up exactly to 1 but should be close
ef.portfolio_performance(verbose=True)

cleaned_weights

"""---

# Calculating Sharpe Ratio (2nd method)
"""

# importing necessary libraries
import ssl # TLS/SSL wrapper for socket objects
from functools import wraps 
import numpy as np # perform math calculations
import pandas as pd # data analysis
import pandas_datareader.data as web # collect data from resources 
import matplotlib.pyplot as plt # draw graphics

# setting up connection
def sslwrap(func):
    @wraps(func)
    def bar(*args, **kw):
        kw['ssl_version'] = ssl.PROTOCOL_TLSv1
        return func(*args, **kw)
    return bar
ssl.wrap_socket = sslwrap(ssl.wrap_socket)

# calculate mean daily return and covariance of daily returns
mean_daily_returns = returns.mean()
cov_matrix = returns.cov()

# set number of runs of random portfolio weights
num_portfolios = 10000
 
# set up array to hold results
# Array to hold weight for each stock
results = np.zeros((3+len(assets),num_portfolios))

for i in range(num_portfolios):
    # select random weights for portfolio holdings
    weights = np.array(np.random.random(10))
    # normalizing weights
    weights /= np.sum(weights)
 
    # calculate portfolio return and volatility(which is a standard deviation)
    portfolio_return = np.sum(mean_daily_returns * weights) * 252
    portfolio_std_dev = np.sqrt(np.dot(weights.T,np.dot(cov_matrix, weights)))*np.sqrt(252)
 
    # store results in results array
    results[0,i] = portfolio_return
    results[1,i] = portfolio_std_dev
    # store Sharpe Ratio (return / volatility) - risk free rate element excluded for simplicity
    results[2,i] = ( results[0,i] )/ (results[1,i] )
    # iterate through the weight vector and add data to results array
    for j in range(0,10):
        results[j+3,i] = weights[j]

results_frame = pd.DataFrame(results.T, columns=['ret','stdev','sharpe',assets[0], assets[1], 
                                                 assets[2], assets[3], assets[4], assets[5], 
                                                 assets[6], assets[7], assets[8], assets[9]])
# locate position of portfolio with highest Sharpe Ratio
maxsp = results_frame.iloc[results_frame[['sharpe']].idxmax()]
# locate positon of portfolio with minimum standard deviation
minvp = results_frame.iloc[results_frame['stdev'].idxmin()]
print(results_frame)
# create scatter plot coloured by Sharpe Ratio
plt.scatter(results_frame.stdev,results_frame.ret,c = results_frame.sharpe,cmap='RdYlBu')
plt.xlabel('Volatility')
plt.ylabel('Returns')
plt.colorbar()

maxsp_array = pd.DataFrame.to_numpy(maxsp) # converting DataFrame to numpy for convenience

# print the line that was chosen as a max Sharpe Ratio
maxsp

# weights for each company in portfolio
print("FB: ", np.round(maxsp_array[0][3],6)," / AMZN: ", np.round(maxsp_array[0][4],6), 
      " / AAPL: ", np.round(maxsp_array[0][5],6), " / GOOG: ", np.round(maxsp_array[0][6],6),'\n'
      "NFLX: ", np.round(maxsp_array[0][7],6), " / TSLA: ", np.round(maxsp_array[0][8],6),
      " / MSFT: ", np.round(maxsp_array[0][9],6), " / IBM: ", np.round(maxsp_array[0][10],6),'\n'
      "ORCL: ", np.round(maxsp_array[0][11],6), " / ABDE: ", np.round(maxsp_array[0][12],6))

print("Expected portfolio return: ", np.round(100*maxsp_array[0][0],2), "%")
print("Expected portfolio volatility: ", np.round(100*np.average(minvp),2), "%")
print("Maximum Sharpe ration of the portfolio: ", np.round(np.max(results[2]),2))

weigh_2sr = maxsp.drop({"ret", "stdev", "sharpe"}, axis=1).transpose()
weigh_2sr.rename(columns = {weigh_2sr.columns[0]: 'weight'}, inplace = True)
weigh_2sr = weigh_2sr['weight']
#weigh_2sr = pd.Series(weigh_2sr.columns[0])
weigh_2sr

"""---

#Comparison between HRP and Sharpe Ratio
"""

print("HRP")
get_calc(hrp)

sharpe_w  = pd.Series(cleaned_weights,index=cleaned_weights.keys())
print("Sharpe Ratio")
get_calc(sharpe_w)

print("Sharpe Ratio Second Method")
get_calc(weigh_2sr)

"""###Additional"""

ivp = getIVP(cov)
ivp = pd.Series(ivp, index=cov.index)

mvp = getMVP(cov)
mvp = pd.Series(mvp, index=cov.index)

print("Traditional risk parity’s Inverse-Variance Portfolio (IVP)")
get_calc(ivp)
print()
print("Markowitz’s Minimum-Variance Portfolio (MVP)")
get_calc(mvp)